{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b565a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_path='../Our Labeled Data/No_Dup/no_d/'\n",
    "contains_dup_path='../Our Labeled Data/Contains_Dup/contains_d/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3f608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_files = os.listdir(no_dup_path)\n",
    "contains_dup_files = os.listdir(contains_dup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65be4a3",
   "metadata": {},
   "source": [
    "### 1. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a84d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    for _ in range(10):\n",
    "        t=tempdf.sample(n=2).reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "        df1= df1.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884968b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((963, 7), (37, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['isDuplicate']==0].shape, df1[df1['isDuplicate']==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4970073a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>total1</th>\n",
       "      <th>total2</th>\n",
       "      <th>isDuplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euro</td>\n",
       "      <td>euro</td>\n",
       "      <td>12319.0</td>\n",
       "      <td>12319.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999 LUF Euro / Euro</td>\n",
       "      <td>lev</td>\n",
       "      <td>534.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chilean peso</td>\n",
       "      <td>new sol</td>\n",
       "      <td>204.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999 ATS euro / euro</td>\n",
       "      <td>Hong Kong dollar</td>\n",
       "      <td>626.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore dollar</td>\n",
       "      <td>US dollar</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Japanese Language Proficiency Test</td>\n",
       "      <td>Bellydance Dance Belly Dancing</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Dance: African Dance and Drumming</td>\n",
       "      <td>Korean Language &amp; Culture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Michigan Sports</td>\n",
       "      <td>Sport Touring Motorcycles</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Paper Crafts</td>\n",
       "      <td>Boomer Wellness and Well-Being</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>LGBT Families San Francisco</td>\n",
       "      <td>Beauty Industry</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     w1                              w2  \\\n",
       "0                                  Euro                            euro   \n",
       "1             1999 LUF Euro / Euro                                  lev   \n",
       "2                          Chilean peso                         new sol   \n",
       "3                  1999 ATS euro / euro                Hong Kong dollar   \n",
       "4                      Singapore dollar                       US dollar   \n",
       "..                                  ...                             ...   \n",
       "995  Japanese Language Proficiency Test  Bellydance Dance Belly Dancing   \n",
       "996   Dance: African Dance and Drumming       Korean Language & Culture   \n",
       "997                     Michigan Sports       Sport Touring Motorcycles   \n",
       "998                        Paper Crafts  Boomer Wellness and Well-Being   \n",
       "999         LGBT Families San Francisco                 Beauty Industry   \n",
       "\n",
       "      count1   count2   total1   total2  isDuplicate  \n",
       "0    12319.0  12319.0  68023.0  68023.0          1.0  \n",
       "1      534.0    534.0  68023.0  68023.0          0.0  \n",
       "2      204.0    204.0  68023.0  68023.0          0.0  \n",
       "3      626.0    626.0  68023.0  68023.0          0.0  \n",
       "4     1944.0   1944.0  68023.0  68023.0          0.0  \n",
       "..       ...      ...      ...      ...          ...  \n",
       "995      1.0      1.0  31212.0  31212.0          0.0  \n",
       "996      3.0      3.0  31212.0  31212.0          0.0  \n",
       "997      1.0      1.0  31212.0  31212.0          0.0  \n",
       "998     16.0     16.0  31212.0  31212.0          0.0  \n",
       "999      1.0      1.0  31212.0  31212.0          0.0  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59727335",
   "metadata": {},
   "source": [
    "### 2. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows with a. same entitity number, b. different entitity number\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c311f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "\n",
    "    for _ in range(3):\n",
    "        tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "        \n",
    "        try:\n",
    "            t=tempdf[tempdf['entityCount']>1].sample(n=1).reset_index()\n",
    "            e=t.loc[0]['Entity_Number']\n",
    "            t=tempdf[tempdf['Entity_Number']==e].sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "#             t=tempdf[tempdf['entityCount']==1].sample(n=2).reset_index()\n",
    "            t=tempdf.sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f18ab227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    314\n",
       "0.0    286\n",
       "Name: isDuplicate, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['isDuplicate'].value_counts()\n",
    "\n",
    "df2_nodup = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76efad",
   "metadata": {},
   "source": [
    "# Aproach 3\n",
    "\n",
    "Get all Pairs of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41dd0a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 351 282 Suburb.csv\n",
      "3 205 355 Suburb.csv\n",
      "1 151 223 Currency Unit.csv\n",
      "56 112 198 foreign_worker_info_state.csv\n",
      "60 213 327 currency.csv\n",
      "5 10 198 wage_offer_unit_of_pay_9089.csv\n",
      "1 3 227 License Type.csv\n",
      "86 738 261 organization.csv\n",
      "15 388 282 SellerG.csv\n",
      "3 291 168 city.csv\n",
      "55 190 328 currency.csv\n",
      "52 108 198 agent_state.csv\n",
      "154 789 355 Colour.csv\n",
      "1 152 131 CountryNumeric2.csv\n",
      "1 152 127 Label.csv\n",
      "1 49 328 item.csv\n",
      "1 384 290 city.csv\n",
      "855 2670 198 employer_phone_ext.csv\n",
      "44 928 190 Venue.csv\n",
      "1 46 241 Household Head Highest Grade Completed.csv\n",
      "4 14 50 EVENT_TYPE.csv\n",
      "7 87 325 currency.csv\n",
      "6 731 225 SeriesCode.csv\n",
      "14 961 163 topic_key.csv\n",
      "6 95 324 currency.csv\n",
      "13 905 198 pw_soc_title.csv\n",
      "2 4 356 Animal_Desexed.csv\n",
      "6 928 35 region.csv\n",
      "5 10 198 pw_unit_of_pay_9089.csv\n",
      "13 40 51 color1.csv\n",
      "7 96 323 currency.csv\n",
      "11 455 89 province.csv\n",
      "56 113 198 job_info_work_state.csv\n",
      "17 397 356 Suburb.csv\n",
      "3 6 356 Gender.csv\n",
      "7 167 266 state.csv\n",
      "331 1217 355 Breed_Description.csv\n",
      "30 293 225 CountryCode.csv\n",
      "154 1169 198 preparer_info_title.csv\n",
      "1 218 222 CountryCode.csv\n",
      "67 223 326 currency.csv\n",
      "2 10 356 Animal_Type.csv\n",
      "9 928 191 Venue.csv\n",
      "13 44 51 coat.csv\n",
      "1 3 291 plant_type.csv\n",
      "3 10 162 venue.city.csv\n",
      "2 4 356 Microchipped.csv\n",
      "22 961 163 topic_name.csv\n",
      "3 55 351 IdentificationQualifier.csv\n",
      "1 11 352 FundingMeth_GASB.csv\n",
      "4 107 293 CITY.csv\n",
      "56 113 198 employer_state.csv\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    print(len(dupEntityNo), file)\n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        df3 = df3.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        df3 = df3.append({'w1':w2, 'w2':w1, 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7405660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c26bd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>total1</th>\n",
       "      <th>total2</th>\n",
       "      <th>isDuplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Viewbank</td>\n",
       "      <td>viewbank</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>viewbank</td>\n",
       "      <td>Viewbank</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Croydon</td>\n",
       "      <td>croydon</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>croydon</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>34857.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>adelaide</td>\n",
       "      <td>360.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>82545.0</td>\n",
       "      <td>82545.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>WV</td>\n",
       "      <td>WEST VIRGINIA</td>\n",
       "      <td>188.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542</th>\n",
       "      <td>WI</td>\n",
       "      <td>WISCONSIN</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4543</th>\n",
       "      <td>WISCONSIN</td>\n",
       "      <td>WI</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>WY</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4545</th>\n",
       "      <td>WY</td>\n",
       "      <td>WYOMING</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4546 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             w1             w2  count1  count2    total1    total2  \\\n",
       "0      Viewbank       viewbank   100.0   100.0   34857.0   34857.0   \n",
       "1      viewbank       Viewbank   100.0   100.0   34857.0   34857.0   \n",
       "2       Croydon        croydon   133.0   133.0   34857.0   34857.0   \n",
       "3       croydon        Croydon   133.0   133.0   34857.0   34857.0   \n",
       "4      Adelaide       adelaide   360.0   360.0   82545.0   82545.0   \n",
       "...         ...            ...     ...     ...       ...       ...   \n",
       "4541         WV  WEST VIRGINIA   188.0   188.0  374362.0  374362.0   \n",
       "4542         WI      WISCONSIN  1826.0  1826.0  374362.0  374362.0   \n",
       "4543  WISCONSIN             WI  1826.0  1826.0  374362.0  374362.0   \n",
       "4544    WYOMING             WY    65.0    65.0  374362.0  374362.0   \n",
       "4545         WY        WYOMING    65.0    65.0  374362.0  374362.0   \n",
       "\n",
       "      isDuplicate  \n",
       "0             1.0  \n",
       "1             1.0  \n",
       "2             1.0  \n",
       "3             1.0  \n",
       "4             1.0  \n",
       "...           ...  \n",
       "4541          1.0  \n",
       "4542          1.0  \n",
       "4543          1.0  \n",
       "4544          1.0  \n",
       "4545          1.0  \n",
       "\n",
       "[4546 rows x 7 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f47c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "for e in dupEntityNo:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    for _ in range(10):\n",
    "        t=tempdf.sample(n=2).reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "        df1= df1.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
