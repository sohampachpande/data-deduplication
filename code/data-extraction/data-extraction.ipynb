{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b565a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_path='../../Our Labeled Data/No_Dup/no_d/'\n",
    "contains_dup_path='../../Our Labeled Data/Contains_Dup/contains_d/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3f608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_files = os.listdir(no_dup_path)\n",
    "contains_dup_files = os.listdir(contains_dup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65be4a3",
   "metadata": {},
   "source": [
    "### 1. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a84d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    for _ in range(10):\n",
    "        t=tempdf.sample(n=2).reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "        df1= df1.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884968b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((963, 7), (37, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['isDuplicate']==0].shape, df1[df1['isDuplicate']==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4970073a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>total1</th>\n",
       "      <th>total2</th>\n",
       "      <th>isDuplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euro</td>\n",
       "      <td>euro</td>\n",
       "      <td>12319.0</td>\n",
       "      <td>12319.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999 LUF Euro / Euro</td>\n",
       "      <td>lev</td>\n",
       "      <td>534.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chilean peso</td>\n",
       "      <td>new sol</td>\n",
       "      <td>204.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999 ATS euro / euro</td>\n",
       "      <td>Hong Kong dollar</td>\n",
       "      <td>626.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore dollar</td>\n",
       "      <td>US dollar</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Japanese Language Proficiency Test</td>\n",
       "      <td>Bellydance Dance Belly Dancing</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Dance: African Dance and Drumming</td>\n",
       "      <td>Korean Language &amp; Culture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Michigan Sports</td>\n",
       "      <td>Sport Touring Motorcycles</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Paper Crafts</td>\n",
       "      <td>Boomer Wellness and Well-Being</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>LGBT Families San Francisco</td>\n",
       "      <td>Beauty Industry</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     w1                              w2  \\\n",
       "0                                  Euro                            euro   \n",
       "1             1999 LUF Euro / Euro                                  lev   \n",
       "2                          Chilean peso                         new sol   \n",
       "3                  1999 ATS euro / euro                Hong Kong dollar   \n",
       "4                      Singapore dollar                       US dollar   \n",
       "..                                  ...                             ...   \n",
       "995  Japanese Language Proficiency Test  Bellydance Dance Belly Dancing   \n",
       "996   Dance: African Dance and Drumming       Korean Language & Culture   \n",
       "997                     Michigan Sports       Sport Touring Motorcycles   \n",
       "998                        Paper Crafts  Boomer Wellness and Well-Being   \n",
       "999         LGBT Families San Francisco                 Beauty Industry   \n",
       "\n",
       "      count1   count2   total1   total2  isDuplicate  \n",
       "0    12319.0  12319.0  68023.0  68023.0          1.0  \n",
       "1      534.0    534.0  68023.0  68023.0          0.0  \n",
       "2      204.0    204.0  68023.0  68023.0          0.0  \n",
       "3      626.0    626.0  68023.0  68023.0          0.0  \n",
       "4     1944.0   1944.0  68023.0  68023.0          0.0  \n",
       "..       ...      ...      ...      ...          ...  \n",
       "995      1.0      1.0  31212.0  31212.0          0.0  \n",
       "996      3.0      3.0  31212.0  31212.0          0.0  \n",
       "997      1.0      1.0  31212.0  31212.0          0.0  \n",
       "998     16.0     16.0  31212.0  31212.0          0.0  \n",
       "999      1.0      1.0  31212.0  31212.0          0.0  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59727335",
   "metadata": {},
   "source": [
    "### 2. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows with a. same entitity number, b. different entitity number\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c311f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "\n",
    "    for _ in range(3):\n",
    "        tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "        \n",
    "        try:\n",
    "            t=tempdf[tempdf['entityCount']>1].sample(n=1).reset_index()\n",
    "            e=t.loc[0]['Entity_Number']\n",
    "            t=tempdf[tempdf['Entity_Number']==e].sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "#             t=tempdf[tempdf['entityCount']==1].sample(n=2).reset_index()\n",
    "            t=tempdf.sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f18ab227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    314\n",
       "0.0    286\n",
       "Name: isDuplicate, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['isDuplicate'].value_counts()\n",
    "\n",
    "df2_nodup = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76efad",
   "metadata": {},
   "source": [
    "# Aproach 3\n",
    "\n",
    "Get all Pairs of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41dd0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        df3 = df3.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        df3 = df3.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b3baeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_nodup = df3.drop_duplicates()\n",
    "df3_nodup = df3_nodup[df3_nodup['w1']!=df3_nodup['w2']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7405660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('allDuplicates.csv')\n",
    "df3_nodup.to_csv('allDuplicates_cleanStrings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ec166ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1782, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_nodup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7f8237e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in no_dup_files:\n",
    "    file_path=no_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples']\n",
    "            w2,c2,t2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples']\n",
    "            isDup=0.0\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "            \n",
    "                df4 = df4.append({'w1':str(w1).strip(), 'w2':str(w2).strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df4 = df4.append({'w1':str(w2).strip(), 'w2':str(w1).strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                \n",
    "                \n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    tempdf = tempdf[tempdf['entityCount']==1]\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "                df3 = df3.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df3 = df3.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1db1adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>total1</th>\n",
       "      <th>total2</th>\n",
       "      <th>isDuplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Month</td>\n",
       "      <td>Year</td>\n",
       "      <td>39.0</td>\n",
       "      <td>68950.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Year</td>\n",
       "      <td>Month</td>\n",
       "      <td>68950.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hour</td>\n",
       "      <td>Bi-Weekly</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bi-Weekly</td>\n",
       "      <td>Hour</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>NV</td>\n",
       "      <td>7354.0</td>\n",
       "      <td>7779.0</td>\n",
       "      <td>403984.0</td>\n",
       "      <td>403984.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9505</th>\n",
       "      <td>VIEW</td>\n",
       "      <td>LIKE</td>\n",
       "      <td>61086.0</td>\n",
       "      <td>5745.0</td>\n",
       "      <td>72312.0</td>\n",
       "      <td>72312.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9506</th>\n",
       "      <td>FOLLOW</td>\n",
       "      <td>COMMENT CREATED</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>1611.0</td>\n",
       "      <td>72312.0</td>\n",
       "      <td>72312.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9507</th>\n",
       "      <td>COMMENT CREATED</td>\n",
       "      <td>FOLLOW</td>\n",
       "      <td>1611.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>72312.0</td>\n",
       "      <td>72312.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>8892.0</td>\n",
       "      <td>8288.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9509</th>\n",
       "      <td>Y</td>\n",
       "      <td>A</td>\n",
       "      <td>8288.0</td>\n",
       "      <td>8892.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>374362.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9510 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   w1               w2   count1   count2    total1    total2  \\\n",
       "0               Month             Year     39.0  68950.0  374362.0  374362.0   \n",
       "1                Year            Month  68950.0     39.0  374362.0  374362.0   \n",
       "2                Hour        Bi-Weekly   1651.0      1.0  374362.0  374362.0   \n",
       "3           Bi-Weekly             Hour      1.0   1651.0  374362.0  374362.0   \n",
       "4                  AL               NV   7354.0   7779.0  403984.0  403984.0   \n",
       "...               ...              ...      ...      ...       ...       ...   \n",
       "9505             VIEW             LIKE  61086.0   5745.0   72312.0   72312.0   \n",
       "9506           FOLLOW  COMMENT CREATED   1407.0   1611.0   72312.0   72312.0   \n",
       "9507  COMMENT CREATED           FOLLOW   1611.0   1407.0   72312.0   72312.0   \n",
       "9508                A                Y   8892.0   8288.0  374362.0  374362.0   \n",
       "9509                Y                A   8288.0   8892.0  374362.0  374362.0   \n",
       "\n",
       "      isDuplicate  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "...           ...  \n",
       "9505          0.0  \n",
       "9506          0.0  \n",
       "9507          0.0  \n",
       "9508          0.0  \n",
       "9509          0.0  \n",
       "\n",
       "[9510 rows x 7 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a3b6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nodup = df4.drop_duplicates()\n",
    "df4_nodup = df4_nodup.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f2b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nodup.to_csv('NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00558c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.concat((df3_nodup, df4_nodup))\n",
    "\n",
    "df5 = df5.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df5.to_csv('1AllDuplicates_5NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef46367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
