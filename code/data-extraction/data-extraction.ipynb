{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b565a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_path='../../Our Labeled Data/No_Dup/no_d/'\n",
    "contains_dup_path='../../Our Labeled Data/Contains_Dup/contains_d/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3f608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_files = os.listdir(no_dup_path)\n",
    "contains_dup_files = os.listdir(contains_dup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65be4a3",
   "metadata": {},
   "source": [
    "### 1. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a84d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    for _ in range(10):\n",
    "        t=tempdf.sample(n=2).reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "        df1= df1.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884968b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((963, 7), (37, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['isDuplicate']==0].shape, df1[df1['isDuplicate']==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4970073a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>total1</th>\n",
       "      <th>total2</th>\n",
       "      <th>isDuplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euro</td>\n",
       "      <td>euro</td>\n",
       "      <td>12319.0</td>\n",
       "      <td>12319.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999 LUF Euro / Euro</td>\n",
       "      <td>lev</td>\n",
       "      <td>534.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chilean peso</td>\n",
       "      <td>new sol</td>\n",
       "      <td>204.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999 ATS euro / euro</td>\n",
       "      <td>Hong Kong dollar</td>\n",
       "      <td>626.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore dollar</td>\n",
       "      <td>US dollar</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>68023.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Japanese Language Proficiency Test</td>\n",
       "      <td>Bellydance Dance Belly Dancing</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Dance: African Dance and Drumming</td>\n",
       "      <td>Korean Language &amp; Culture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Michigan Sports</td>\n",
       "      <td>Sport Touring Motorcycles</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Paper Crafts</td>\n",
       "      <td>Boomer Wellness and Well-Being</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>LGBT Families San Francisco</td>\n",
       "      <td>Beauty Industry</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>31212.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     w1                              w2  \\\n",
       "0                                  Euro                            euro   \n",
       "1             1999 LUF Euro / Euro                                  lev   \n",
       "2                          Chilean peso                         new sol   \n",
       "3                  1999 ATS euro / euro                Hong Kong dollar   \n",
       "4                      Singapore dollar                       US dollar   \n",
       "..                                  ...                             ...   \n",
       "995  Japanese Language Proficiency Test  Bellydance Dance Belly Dancing   \n",
       "996   Dance: African Dance and Drumming       Korean Language & Culture   \n",
       "997                     Michigan Sports       Sport Touring Motorcycles   \n",
       "998                        Paper Crafts  Boomer Wellness and Well-Being   \n",
       "999         LGBT Families San Francisco                 Beauty Industry   \n",
       "\n",
       "      count1   count2   total1   total2  isDuplicate  \n",
       "0    12319.0  12319.0  68023.0  68023.0          1.0  \n",
       "1      534.0    534.0  68023.0  68023.0          0.0  \n",
       "2      204.0    204.0  68023.0  68023.0          0.0  \n",
       "3      626.0    626.0  68023.0  68023.0          0.0  \n",
       "4     1944.0   1944.0  68023.0  68023.0          0.0  \n",
       "..       ...      ...      ...      ...          ...  \n",
       "995      1.0      1.0  31212.0  31212.0          0.0  \n",
       "996      3.0      3.0  31212.0  31212.0          0.0  \n",
       "997      1.0      1.0  31212.0  31212.0          0.0  \n",
       "998     16.0     16.0  31212.0  31212.0          0.0  \n",
       "999      1.0      1.0  31212.0  31212.0          0.0  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59727335",
   "metadata": {},
   "source": [
    "### 2. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows with a. same entitity number, b. different entitity number\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c311f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "\n",
    "    for _ in range(3):\n",
    "        tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "        \n",
    "        try:\n",
    "            t=tempdf[tempdf['entityCount']>1].sample(n=1).reset_index()\n",
    "            e=t.loc[0]['Entity_Number']\n",
    "            t=tempdf[tempdf['Entity_Number']==e].sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "#             t=tempdf[tempdf['entityCount']==1].sample(n=2).reset_index()\n",
    "            t=tempdf.sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f18ab227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    314\n",
       "0.0    286\n",
       "Name: isDuplicate, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['isDuplicate'].value_counts()\n",
    "\n",
    "df2_nodup = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76efad",
   "metadata": {},
   "source": [
    "# Aproach 3\n",
    "\n",
    "Get all Pairs of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41dd0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        df3 = df3.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        df3 = df3.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b3baeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_nodup = df3.drop_duplicates()\n",
    "df3_nodup = df3_nodup[df3_nodup['w1']!=df3_nodup['w2']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7405660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('allDuplicates.csv')\n",
    "df3_nodup.to_csv('allDuplicates_cleanStrings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ec166ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1782, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_nodup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7f8237e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in no_dup_files:\n",
    "    file_path=no_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples']\n",
    "            w2,c2,t2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples']\n",
    "            isDup=0.0\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "            \n",
    "                df4 = df4.append({'w1':str(w1).strip(), 'w2':str(w2).strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df4 = df4.append({'w1':str(w2).strip(), 'w2':str(w1).strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                \n",
    "                \n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    tempdf = tempdf[tempdf['entityCount']==1]\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "                df4 = df4.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df4 = df4.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "df4_nodup = df4.drop_duplicates()\n",
    "df4_nodup = df4_nodup.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f2b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nodup.to_csv('NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00558c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.concat((df3_nodup, df4_nodup))\n",
    "\n",
    "df5 = df5.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df5.to_csv('1AllDuplicates_5NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef46367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb32da6a",
   "metadata": {},
   "source": [
    "# Aproach 4\n",
    "\n",
    "### Hold out files from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "082b5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains_dup_files\n",
    "\n",
    "record_ids = []\n",
    "for f in contains_dup_files:\n",
    "    record_ids.append(f.split()[1])\n",
    "\n",
    "trainRecordID = random.choices(list(set(record_ids)), k = 25)\n",
    "testRecordID = list(set(record_ids) - set(trainFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5b286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_dup_file_HoldOutTrain = []\n",
    "contains_dup_file_HoldOutTest = []\n",
    "\n",
    "for f in contains_dup_files:\n",
    "    if f.split()[1] in trainRecordID:\n",
    "        contains_dup_file_HoldOutTrain.append(f)\n",
    "    else:\n",
    "        contains_dup_file_HoldOutTest.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168171d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22d18823",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOutTrainDup = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "total = 0\n",
    "for file in contains_dup_file_HoldOutTrain:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        dfHoldOutTrainDup = dfHoldOutTrainDup.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)    \n",
    "        dfHoldOutTrainDup = dfHoldOutTrainDup.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "dfHoldOutTrainDup = dfHoldOutTrainDup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4b6d79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2901, 7)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfHoldOutTrainDup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "230c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOutTestDup = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_file_HoldOutTest:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        dfHoldOutTestDup = dfHoldOutTestDup.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        dfHoldOutTestDup = dfHoldOutTestDup.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "\n",
    "dfHoldOutTestDup = dfHoldOutTestDup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d82ce50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1373, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfHoldOutTestDup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86e73049",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLen, testLen = dfHoldOutTrainDup.shape[0], dfHoldOutTestDup.shape[0]\n",
    "totalLen = trainLen+testLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1832aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HoldOutTrainData = pd.concat([dfHoldOutTrainDup, df4[:int(df4.shape[0]*(trainLen/totalLen))]])\n",
    "HoldOutTestData = pd.concat([dfHoldOutTestDup, df4[int(df4.shape[0]*(trainLen/totalLen)):]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74abdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HoldOutTrainData = HoldOutTrainData.drop_duplicates()\n",
    "HoldOutTrainData = HoldOutTrainData.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "HoldOutTestData = HoldOutTestData.drop_duplicates()\n",
    "HoldOutTestData = HoldOutTestData.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "HoldOutTrainData.to_csv(\"HoldOutTrainData.csv\")\n",
    "HoldOutTestData.to_csv(\"HoldOutTestData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4fe986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"trainRecordID\":trainRecordID}).to_csv(\"trainRecordIDs.csv\")\n",
    "\n",
    "pd.DataFrame({\"testRecordID\":testRecordID}).to_csv(\"testRecordIDs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548b447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
