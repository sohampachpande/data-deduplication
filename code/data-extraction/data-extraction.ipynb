{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b565a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_path='../../Our Labeled Data/No_Dup/no_d/'\n",
    "contains_dup_path='../../Our Labeled Data/Contains_Dup/contains_d/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3f608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_files = os.listdir(no_dup_path)\n",
    "contains_dup_files = os.listdir(contains_dup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65be4a3",
   "metadata": {},
   "source": [
    "### 1. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a84d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    for _ in range(10):\n",
    "        t=tempdf.sample(n=2).reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "        df1= df1.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884968b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((963, 7), (37, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['isDuplicate']==0].shape, df1[df1['isDuplicate']==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4970073a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_Set</th>\n",
       "      <th>Category_Occurrences</th>\n",
       "      <th>Total_Examples</th>\n",
       "      <th>Entity_Number</th>\n",
       "      <th>entityCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California</td>\n",
       "      <td>44508</td>\n",
       "      <td>150930</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>9750</td>\n",
       "      <td>150930</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tuscany</td>\n",
       "      <td>7281</td>\n",
       "      <td>150930</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>6111</td>\n",
       "      <td>150930</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>4892</td>\n",
       "      <td>150930</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Malgas</td>\n",
       "      <td>1</td>\n",
       "      <td>150930</td>\n",
       "      <td>448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Viile Timis</td>\n",
       "      <td>1</td>\n",
       "      <td>150930</td>\n",
       "      <td>449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Vale Trentino</td>\n",
       "      <td>1</td>\n",
       "      <td>150930</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Cape South Coast</td>\n",
       "      <td>1</td>\n",
       "      <td>150930</td>\n",
       "      <td>451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Slovenska Istra</td>\n",
       "      <td>1</td>\n",
       "      <td>150930</td>\n",
       "      <td>453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Category_Set  Category_Occurrences  Total_Examples  Entity_Number  \\\n",
       "0          California                 44508          150930              1   \n",
       "1          Washington                  9750          150930              2   \n",
       "2             Tuscany                  7281          150930              3   \n",
       "3            Bordeaux                  6111          150930              4   \n",
       "4      Northern Spain                  4892          150930              5   \n",
       "..                ...                   ...             ...            ...   \n",
       "450            Malgas                     1          150930            448   \n",
       "451       Viile Timis                     1          150930            449   \n",
       "452     Vale Trentino                     1          150930            450   \n",
       "453  Cape South Coast                     1          150930            451   \n",
       "454   Slovenska Istra                     1          150930            453   \n",
       "\n",
       "     entityCount  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "..           ...  \n",
       "450            1  \n",
       "451            1  \n",
       "452            1  \n",
       "453            1  \n",
       "454            1  \n",
       "\n",
       "[455 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59727335",
   "metadata": {},
   "source": [
    "### 2. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows with a. same entitity number, b. different entitity number\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83c311f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'isDuplicate':[]})\n",
    "# df2 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "\n",
    "    for _ in range(3):\n",
    "        tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "        \n",
    "        try:\n",
    "            t=tempdf[tempdf['entityCount']>1].sample(n=1).reset_index()\n",
    "            e=t.loc[0]['Entity_Number']\n",
    "            t=tempdf[tempdf['Entity_Number']==e].sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "            # df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "#             t=tempdf[tempdf['entityCount']==1].sample(n=2).reset_index()\n",
    "            t=tempdf.sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "            # df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f18ab227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['isDuplicate'].value_counts()\n",
    "\n",
    "df2_nodup = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>isDuplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>U.S. dollar</td>\n",
       "      <td>U. S. dollar</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>U.S. dollar</td>\n",
       "      <td>U. S. dollar</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              w1            w2  count1  count2  isDuplicate\n",
       "296  U.S. dollar  U. S. dollar    15.0     1.0          1.0\n",
       "298  U.S. dollar  U. S. dollar    15.0     1.0          1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2[\"w1\"]==\"U.S. dollar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76efad",
   "metadata": {},
   "source": [
    "# Aproach 3\n",
    "\n",
    "Get all Pairs of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41dd0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        df3 = df3.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        df3 = df3.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b3baeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_nodup = df3.drop_duplicates()\n",
    "df3_nodup = df3_nodup[df3_nodup['w1']!=df3_nodup['w2']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7405660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('allDuplicates.csv')\n",
    "df3_nodup.to_csv('allDuplicates_cleanStrings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ec166ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1782, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_nodup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7f8237e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in no_dup_files:\n",
    "    file_path=no_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples']\n",
    "            w2,c2,t2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples']\n",
    "            isDup=0.0\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "            \n",
    "                df4 = df4.append({'w1':str(w1).strip(), 'w2':str(w2).strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df4 = df4.append({'w1':str(w2).strip(), 'w2':str(w1).strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                \n",
    "                \n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    tempdf = tempdf[tempdf['entityCount']==1]\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "                df4 = df4.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df4 = df4.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "df4_nodup = df4.drop_duplicates()\n",
    "df4_nodup = df4_nodup.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f2b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nodup.to_csv('NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00558c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.concat((df3_nodup, df4_nodup))\n",
    "\n",
    "df5 = df5.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df5.to_csv('1AllDuplicates_5NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef46367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb32da6a",
   "metadata": {},
   "source": [
    "# Aproach 4\n",
    "\n",
    "### Hold out files from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "082b5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains_dup_files\n",
    "\n",
    "record_ids = []\n",
    "for f in contains_dup_files:\n",
    "    record_ids.append(f.split()[1])\n",
    "\n",
    "trainRecordID = random.choices(list(set(record_ids)), k = 25)\n",
    "testRecordID = list(set(record_ids) - set(trainFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5b286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_dup_file_HoldOutTrain = []\n",
    "contains_dup_file_HoldOutTest = []\n",
    "\n",
    "for f in contains_dup_files:\n",
    "    if f.split()[1] in trainRecordID:\n",
    "        contains_dup_file_HoldOutTrain.append(f)\n",
    "    else:\n",
    "        contains_dup_file_HoldOutTest.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168171d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22d18823",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOutTrainDup = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "total = 0\n",
    "for file in contains_dup_file_HoldOutTrain:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        dfHoldOutTrainDup = dfHoldOutTrainDup.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)    \n",
    "        dfHoldOutTrainDup = dfHoldOutTrainDup.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "dfHoldOutTrainDup = dfHoldOutTrainDup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4b6d79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2901, 7)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfHoldOutTrainDup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "230c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOutTestDup = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_file_HoldOutTest:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        dfHoldOutTestDup = dfHoldOutTestDup.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        dfHoldOutTestDup = dfHoldOutTestDup.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "\n",
    "dfHoldOutTestDup = dfHoldOutTestDup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d82ce50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1373, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfHoldOutTestDup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86e73049",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLen, testLen = dfHoldOutTrainDup.shape[0], dfHoldOutTestDup.shape[0]\n",
    "totalLen = trainLen+testLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1832aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HoldOutTrainData = pd.concat([dfHoldOutTrainDup, df4[:int(df4.shape[0]*(trainLen/totalLen))]])\n",
    "HoldOutTestData = pd.concat([dfHoldOutTestDup, df4[int(df4.shape[0]*(trainLen/totalLen)):]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74abdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HoldOutTrainData = HoldOutTrainData.drop_duplicates()\n",
    "HoldOutTrainData = HoldOutTrainData.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "HoldOutTestData = HoldOutTestData.drop_duplicates()\n",
    "HoldOutTestData = HoldOutTestData.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "HoldOutTrainData.to_csv(\"HoldOutTrainData.csv\")\n",
    "HoldOutTestData.to_csv(\"HoldOutTestData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4fe986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"trainRecordID\":trainRecordID}).to_csv(\"trainRecordIDs.csv\")\n",
    "\n",
    "pd.DataFrame({\"testRecordID\":testRecordID}).to_csv(\"testRecordIDs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548b447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2db466ff52d66022f9f91c796fc91b92f6c42d56a30aade30d4e70cac3fc604"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
