{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "np.random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b565a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_path='../Our Labeled Data/No_Dup/no_d/'\n",
    "contains_dup_path='../Our Labeled Data/Contains_Dup/contains_d/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3f608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_files = os.listdir(no_dup_path)\n",
    "contains_dup_files = os.listdir(contains_dup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65be4a3",
   "metadata": {},
   "source": [
    "### 1. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a84d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    for _ in range(10):\n",
    "        t=tempdf.sample(n=2).reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "        df1= df1.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59727335",
   "metadata": {},
   "source": [
    "### 2. Approach to create labeled data with features - word1, word2, isDuplicate\n",
    "\n",
    "1. Randomly pick one file from the contains_duplicate dataset\n",
    "2. Randomly pick two rows with a. same entitity number, b. different entitity number\n",
    "3. Populate the output data with words and isDuplicate value according to the entity number from the two pairs of rows\n",
    "\n",
    "\n",
    "In this approach we completely ignore the data statistics and context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c311f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'isDuplicate':[]})\n",
    "# df2 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "for _ in range(100):\n",
    "    file=random.choice(contains_dup_files)\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "\n",
    "    for _ in range(3):\n",
    "        tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "        \n",
    "        try:\n",
    "            t=tempdf[tempdf['entityCount']>1].sample(n=1).reset_index()\n",
    "            e=t.loc[0]['Entity_Number']\n",
    "            t=tempdf[tempdf['Entity_Number']==e].sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "            # df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "#             t=tempdf[tempdf['entityCount']==1].sample(n=2).reset_index()\n",
    "            t=tempdf.sample(n=2).reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "            # df2 = df2.append({'w1':w1, 'w2':w2, 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f18ab227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "df2 = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>isDuplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>U.S. dollar</td>\n",
       "      <td>U. S. dollar</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              w1            w2  count1  count2  isDuplicate\n",
       "324  U.S. dollar  U. S. dollar    15.0     1.0          1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "df2[df2[\"w1\"]==\"U.S. dollar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76efad",
   "metadata": {},
   "source": [
    "# Aproach 3\n",
    "\n",
    "Get all Pairs of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41dd0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        df3 = df3.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        df3 = df3.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b3baeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_nodup = df3.drop_duplicates()\n",
    "df3_nodup = df3_nodup[df3_nodup['w1']!=df3_nodup['w2']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7405660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_nodup.to_csv('allDuplicates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Randomly sampled Non Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7f8237e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in no_dup_files:\n",
    "    file_path=no_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples']\n",
    "            w2,c2,t2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples']\n",
    "            isDup=0.0\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "            \n",
    "                df4 = df4.append({'w1':str(w1).strip(), 'w2':str(w2).strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df4 = df4.append({'w1':str(w2).strip(), 'w2':str(w1).strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                \n",
    "                \n",
    "for file in contains_dup_files:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    tempdf = tempdf[tempdf['entityCount']==1]\n",
    "    \n",
    "    for _ in range(20):\n",
    "        if len(tempdf)>2:\n",
    "            t=tempdf.sample(n=2)\n",
    "            tempdf = tempdf.drop(t.index)\n",
    "            t = t.reset_index()\n",
    "            w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "            w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "            isDup=(e1==e2)\n",
    "            \n",
    "            if type(w1)==str and type(w2)==str: \n",
    "                df4 = df4.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "                df4 = df4.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "\n",
    "df4_nodup = df4.drop_duplicates()\n",
    "df4_nodup = df4_nodup.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f2b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_nodup.to_csv('NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Duplicates and Non Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00558c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.concat((df3_nodup, df4_nodup))\n",
    "\n",
    "df5 = df5.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df5.to_csv('1AllDuplicates5NoDuplicates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32da6a",
   "metadata": {},
   "source": [
    "# Aproach 4\n",
    "\n",
    "### Hold out files from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "082b5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains_dup_files\n",
    "\n",
    "record_ids = []\n",
    "for f in contains_dup_files:\n",
    "    record_ids.append(f.split()[1])\n",
    "\n",
    "trainRecordID = random.choices(list(set(record_ids)), k = 25)\n",
    "testRecordID = list(set(record_ids) - set(trainRecordID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5b286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_dup_file_HoldOutTrain = []\n",
    "contains_dup_file_HoldOutTest = []\n",
    "\n",
    "for f in contains_dup_files:\n",
    "    if f.split()[1] in trainRecordID:\n",
    "        contains_dup_file_HoldOutTrain.append(f)\n",
    "    else:\n",
    "        contains_dup_file_HoldOutTest.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22d18823",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOutTrainDup = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "total = 0\n",
    "for file in contains_dup_file_HoldOutTrain:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        dfHoldOutTrainDup = dfHoldOutTrainDup.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)    \n",
    "        dfHoldOutTrainDup = dfHoldOutTrainDup.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "dfHoldOutTrainDup = dfHoldOutTrainDup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "230c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoldOutTestDup = pd.DataFrame({'w1':[], 'w2':[], 'count1':[], 'count2':[], 'total1':[], 'total2':[], 'isDuplicate':[]})\n",
    "\n",
    "# contains_dup_path\n",
    "\n",
    "total = 0\n",
    "\n",
    "for file in contains_dup_file_HoldOutTest:\n",
    "    file_path=contains_dup_path+file\n",
    "    tempdf=pd.read_csv(file_path)\n",
    "    perEntityCatCount=tempdf['Entity_Number'].value_counts()\n",
    "    \n",
    "    tempdf['entityCount']=tempdf['Entity_Number'].apply(lambda x: perEntityCatCount[x])\n",
    "    \n",
    "    dupEntityNo = set((tempdf[tempdf['entityCount']>1])['Entity_Number'])\n",
    "    \n",
    "    total+=len(dupEntityNo)\n",
    "    \n",
    "    for eNo in dupEntityNo:\n",
    "        t = tempdf[tempdf['Entity_Number']==eNo].reset_index()\n",
    "        w1,c1,t1,e1=t.loc[0]['Category_Set'], t.loc[0]['Category_Occurrences'], t.loc[0]['Total_Examples'], t.loc[0]['Entity_Number']\n",
    "        w2,c2,t2,e2=t.loc[1]['Category_Set'], t.loc[1]['Category_Occurrences'], t.loc[1]['Total_Examples'], t.loc[1]['Entity_Number']\n",
    "        isDup=(e1==e2)\n",
    "    \n",
    "        dfHoldOutTestDup = dfHoldOutTestDup.append({'w1':w1.strip(), 'w2':w2.strip(), 'count1':c1, 'count2':c2, 'total1':t1, 'total2': t2, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "        dfHoldOutTestDup = dfHoldOutTestDup.append({'w1':w2.strip(), 'w2':w1.strip(), 'count1':c2, 'count2':c1, 'total1':t2, 'total2': t1, 'isDuplicate':isDup}, ignore_index=True)\n",
    "    \n",
    "\n",
    "dfHoldOutTestDup = dfHoldOutTestDup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Duplicates and non duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1832aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLen, testLen = dfHoldOutTrainDup.shape[0], dfHoldOutTestDup.shape[0]\n",
    "totalLen = trainLen+testLen\n",
    "\n",
    "HoldOutTrainData = pd.concat([dfHoldOutTrainDup, df4[:int(df4.shape[0]*(trainLen/totalLen))]])\n",
    "HoldOutTestData = pd.concat([dfHoldOutTestDup, df4[int(df4.shape[0]*(trainLen/totalLen)):]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74abdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HoldOutTrainData = HoldOutTrainData.drop_duplicates()\n",
    "HoldOutTrainData = HoldOutTrainData.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "HoldOutTestData = HoldOutTestData.drop_duplicates()\n",
    "HoldOutTestData = HoldOutTestData.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "HoldOutTrainData.to_csv(\"HoldOutTrainData.csv\")\n",
    "HoldOutTestData.to_csv(\"HoldOutTestData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4fe986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"trainRecordID\":trainRecordID}).to_csv(\"trainRecordIDs.csv\")\n",
    "\n",
    "pd.DataFrame({\"testRecordID\":testRecordID}).to_csv(\"testRecordIDs.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2db466ff52d66022f9f91c796fc91b92f6c42d56a30aade30d4e70cac3fc604"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
